{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo vecinos mas cercanos mediante K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "procesando.. data comp.graphics\n",
      "procesando.. data alt.atheism\n",
      "Creando bolsa de palabras de .. comp.graphics\n",
      "Creando bolsa de palabras de .. alt.atheism\n",
      "procesando.. data alt.atheism\n",
      "Creando bolsa de palabras de .. alt.atheism\n",
      "Predicionedo ...\n",
      "Predicionedo ...\n",
      "comp.graphics : 40128991\n",
      "alt.atheism : 15586439\n",
      "La clase a la cual pertenece es  comp.graphics\n"
     ]
    }
   ],
   "source": [
    "#x= open(\"/home/echartea/Descargas/20news-19997/20_newsgroups/Cris.txt\",\"r\");\n",
    "#print(x.readline())\n",
    "import numpy as np\n",
    "import locale\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#SE INICIALIZAN LAS CATEGORIAS\n",
    "categorias=['comp.graphics','alt.atheism',\n",
    " 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'comp.windows.x',\n",
    " 'misc.forsale',\n",
    " 'rec.autos',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'rec.sport.hockey',\n",
    " 'sci.crypt',\n",
    " 'sci.electronics',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.politics.misc',\n",
    " 'talk.religion.misc']\n",
    "\n",
    "lista_train = []#se guarda el texto de la categoria n\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#se debe de crear un objeto por cada clase con el fin de guardar los datos de cada carpeta \n",
    "rango=2#len(categorias)\n",
    "for i in range(0,rango):\n",
    "    print(\"procesando.. data\",categorias[i])\n",
    "    n = fetch_20newsgroups(subset='train',remove=('headers','footers','quotes'),categories=[categorias[i]])\n",
    "    lista_train.append(n)\n",
    "\n",
    "#Me trae los datos limpios de encabezados, pie de paginas y comillas\n",
    "#newsgroups_train = fetch_20newsgroups(subset='train',remove=('headers','footers','quotes'),categories=categorias)\n",
    "#imprimimos los target\n",
    "#print(list(newsgroups_train.target_names))\n",
    "\n",
    "\n",
    "#convertimos el texto en vectores\n",
    "vectorizer = []\n",
    "vectors= []#Se crea una nueva lista para crear las bolsas de palabras\n",
    "for i in range(0,rango):\n",
    "    print(\"Creando bolsa de palabras de ..\",categorias[i])\n",
    "    vectorizer.append(TfidfVectorizer())\n",
    "    vectors.append(vectorizer[i].fit_transform(lista_train[i].data))\n",
    "\n",
    "\n",
    "\n",
    "#print(vectors.shape)\n",
    "\n",
    "\n",
    "##EXTRAER LOS DATOS DE PRUEBA\n",
    "#lista_test=[]\n",
    "#for i in range(0,rango):\n",
    "#    print(\"procesando.. data\",categorias[i])\n",
    "#    n = fetch_20newsgroups(subset='test',remove=('headers','footers','quotes'),categories=[categorias[i]])\n",
    "#    lista_test.append(n)\n",
    "\n",
    "prueba=1;\n",
    "\n",
    "print(\"procesando.. data\",categorias[prueba])\n",
    "n = fetch_20newsgroups(subset='test',remove=('headers','footers','quotes'),categories=[categorias[prueba]])\n",
    "lista_test.append(n)\n",
    "\n",
    "#convertimos el texto en vectores\n",
    "vectorizer_test = []\n",
    "vectors_test= []#Se crea una nueva lista para crear las bolsas de palabras\n",
    "\n",
    "\n",
    "print(\"Creando bolsa de palabras de ..\",categorias[prueba])\n",
    "vectorizer_test.append(TfidfVectorizer())\n",
    "vectors_test.append(vectorizer_test[0].fit_transform(lista_test[0].data))\n",
    "#print(vectors.shape)\n",
    "\n",
    "#for i in range(0,1):\n",
    "#    print(\"Creando bolsa de palabras de ..\",categorias[i])\n",
    "#    vectorizer_test.append(TfidfVectorizer())\n",
    "#    vectors_test.append(vectorizer_test[i].fit_transform(lista_test[i].data))\n",
    "#print(vectors.shape)\n",
    "\n",
    "\n",
    "#metodo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Convertimos el texto de train en vector de palabras\n",
    "texto_clase_test= str(vectorizer_test[0].vocabulary_)#Se convierte todo el texto de la clase en string\n",
    "texto_clase_test= texto_clase_test.replace(\"{\",\"\")#Se eliminan los corchetes\n",
    "texto_clase_test= texto_clase_test.replace(\"}\",\"\")#Se eliminan los corchetes\n",
    "vector_palabras_pesos_test= texto_clase_test.split(\",\")#se separan las palabras por comas\n",
    "\n",
    "contador=0;\n",
    "\n",
    "#Se crea el vector contador\n",
    "contador = [];\n",
    "\n",
    "for i in range(0,rango):\n",
    "    contador.append(0);\n",
    "\n",
    "for i in range(0,rango):#for que recorre las clases de train\n",
    "    print(\"Predicionedo ...\")\n",
    "    #Convertimos el texto de train en vector de palabras\n",
    "    texto_clase_trian= str(vectorizer[i].vocabulary_)#Se convierte todo el texto de la clase en string\n",
    "    texto_clase_trian= texto_clase_trian.replace(\"{\",\"\")#Se eliminan los corchetes\n",
    "    texto_clase_trian= texto_clase_trian.replace(\"}\",\"\")#Se eliminan los corchetes\n",
    "    vector_palabras_pesos_train= texto_clase_trian.split(\",\")#se separan las palabras por comas  \n",
    "    \n",
    "    for j in range(0,1): #for que recorre el test\n",
    "        for m in range(0, len(vector_palabras_pesos_test)):#se recorren todaas las palabras del archivo\n",
    "            #se divide el peso y la palabra test\n",
    "            palabra_test= vector_palabras_pesos_test[m];\n",
    "            palabra_test= palabra_test.split(\",\");\n",
    "            palabra_test= palabra_test[0].split(\":\")\n",
    "            for n in range(0, len(vector_palabras_pesos_train)):\n",
    "                #se divide el peso y la palabra train\n",
    "                palabra_train= vector_palabras_pesos_train[n];\n",
    "                palabra_train= palabra_train.split(\",\");\n",
    "                palabra_train= palabra_train[0].split(\":\")\n",
    "                #print(palabra_train[1])\n",
    "                if(palabra_train[0]==palabra_test[0]):\n",
    "                    #quiere decir que tienen la misma palabra, por lo tanto se debe de aumentar el contador\n",
    "                    contador[i]=int(palabra_train[1])+contador[i];\n",
    "                    #print(palabra_train[0],\" == \", palabra_test[0])\n",
    "    \n",
    "\n",
    "\n",
    "    #determinamos que numero es el mayor.\n",
    "    \n",
    "Nmayor=0\n",
    "Nposicion=0;\n",
    "    \n",
    "for i in range(0, rango):\n",
    "    for j in range(1, rango):\n",
    "        if(contador[i]>contador[j]):\n",
    "            Nposicion=i;\n",
    "            Nmayor=contador[i]\n",
    "            \n",
    "for i in range(0, rango):\n",
    "    print(categorias[i],\":\",contador[i]);\n",
    "    \n",
    "print(\"La clase a la cual pertenece es \", categorias[Nposicion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
